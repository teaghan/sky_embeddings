{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade69e3-5797-4e8a-926d-ea5df1fec01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from astropy.visualization import simple_norm\n",
    "from multiprocessing import Value\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from skimage.segmentation import find_boundaries\n",
    "import time\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.plotting_fns import find_percentile_from_target, rgb_image, desaturate\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "\n",
    "class MaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=(224, 224),\n",
    "        patch_size=16,\n",
    "        enc_mask_scale=(0.2, 0.8),\n",
    "        pred_mask_scale=(0.2, 0.8),\n",
    "        aspect_ratio=(0.3, 3.0),\n",
    "        nenc=1,\n",
    "        npred=2,\n",
    "        min_keep=4,\n",
    "        allow_overlap=False\n",
    "    ):\n",
    "        super(MaskCollator, self).__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  # minimum number of patches to keep\n",
    "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "        self.generator = torch.Generator()  # Initialize a generator\n",
    "        self.generator.manual_seed(int(time.time()))  # Seed the generator once\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        # -- Sample block scale\n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "        # -- Sample block aspect-ratio\n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        # -- Compute block height and width (given scale and aspect-ratio)\n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "        # --\n",
    "        # -- Loop to sample masks until we find a valid one\n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            # -- Sample block top-left corner\n",
    "            top = torch.randint(0, self.height - h + 1, (1,))\n",
    "            left = torch.randint(0, self.width - w + 1, (1,))\n",
    "            max_top = self.height - h\n",
    "            max_left = self.width - w\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            # -- Constrain mask to a set of acceptable regions\n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "            # -- If mask too small try again\n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    print(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        collated_batch = torch.utils.data.default_collate(batch)\n",
    "        \n",
    "        if len(collated_batch) == 2:\n",
    "            collated_images = collated_batch[0]\n",
    "            collated_metadata = collated_batch[1]\n",
    "        else:\n",
    "            collated_images = collated_batch\n",
    "            collated_metadata = torch.zeros(len(collated_batch))\n",
    "        B = len(batch)\n",
    "        \n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            \n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions = None\n",
    "            except Exception as e:\n",
    "                print(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_images, collated_metadata, collated_masks_enc, collated_masks_pred\n",
    "\n",
    "def load_images(batch_size=4, img_size=224):\n",
    "    # Transform to normalize the data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR10 images\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    # Fetch one batch of images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _ = next(dataiter)\n",
    "    return list(images)\n",
    "\n",
    "def upscale_mask(mask, patch_size):\n",
    "    upscaled_mask = mask.repeat_interleave(patch_size, dim=0)\n",
    "    upscaled_mask = upscaled_mask.repeat_interleave(patch_size, dim=1)\n",
    "    return upscaled_mask\n",
    "\n",
    "\n",
    "\n",
    "def read_h5(cutout_dir):\n",
    "    \"\"\"\n",
    "    Reads cutout data from HDF5 file\n",
    "    :param cutout_dir: cutout directory\n",
    "    :return: cutout data\n",
    "    \"\"\"\n",
    "    with h5py.File(cutout_dir, 'r') as f:\n",
    "        # Create empty dictionaries to store data for each group\n",
    "        cutout_data = {}\n",
    "\n",
    "        # Loop through datasets\n",
    "        for dataset_name in f:\n",
    "            data = np.array(f[dataset_name])\n",
    "            cutout_data[dataset_name] = data\n",
    "    return cutout_data\n",
    "\n",
    "class RealTileDataset(IterableDataset):\n",
    "    def __init__(self, cutouts, metadata, crop_size=(224,224)):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with real data.\n",
    "\n",
    "        Args:\n",
    "        cutouts (Tensor): A tensor or list of image data.\n",
    "        metadata (Tensor): A tensor or list of corresponding metadata.\n",
    "        \"\"\"\n",
    "        \n",
    "        cutouts = torch.tensor(cutouts, dtype=torch.float32)\n",
    "        self.cutouts = cutouts\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(crop_size),\n",
    "        ])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for cutout, meta in zip(self.cutouts, self.metadata):\n",
    "            cutout = self.transform(cutout)\n",
    "            yield cutout, meta\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of items in the dataset\n",
    "        return len(self.cutouts)\n",
    "\n",
    "def convert_to_binary(input_string):\n",
    "    # Define the standard order of letters\n",
    "    standard_letters = 'ugriz'\n",
    "    # Initialize the result as an empty string\n",
    "    result = ''\n",
    "    # Iterate over each letter in the standard order\n",
    "    for letter in standard_letters:\n",
    "        # Append '2' if the letter is in the input string, '1' otherwise\n",
    "        if letter in input_string:\n",
    "            result += '2'\n",
    "        else:\n",
    "            result += '1'\n",
    "    # Convert the binary string to a decimal integer\n",
    "    return np.int32(result)\n",
    "\n",
    "\n",
    "def split_tile_nums(df):\n",
    "    # Split the tuple into two separate columns\n",
    "    if isinstance(df['tile'][0], str):\n",
    "        df['tile'] = df['tile'].apply(ast.literal_eval)\n",
    "    df['tile_num1'], df['tile_num2'] = zip(*df['tile'])\n",
    "    # Drop tile column\n",
    "    df.drop('tile', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tensor_compatible(df):\n",
    "    # Convert band info to integer number, 2 -> present, 1 -> absent\n",
    "    if isinstance(df['bands'][0], str):\n",
    "        df['bands'] = df['bands'].apply(convert_to_binary)\n",
    "    if 'tile' in df.columns:\n",
    "        # Split tile numbers up to two different columns and delete the tile column\n",
    "        df = split_tile_nums(df)\n",
    "    return df\n",
    "\n",
    "def get_band_indices(bands, bands_rgb):\n",
    "    bands_full = ['G', 'I', 'R', 'Y', 'Z', 'NB0387', 'NB0816', 'NB0921', 'NB1010']\n",
    "    # bands_full = ['U', 'G', 'R', 'I', 'Z']\n",
    "    band_idx = sorted([bands_full.index(band) for band in bands])\n",
    "    band_idx_rgb = sorted([bands.index(band) for band in bands_rgb])\n",
    "    if len(band_idx) == 0:\n",
    "        logger.error('Band index list is empty.')\n",
    "    return band_idx, band_idx_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa139379-c918-477c-bb36-30d80d8cabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(images, masks_enc, masks_pred, patch_size, bands, alpha_grid, num_samples=5, patch_grid=False, savename=None, show_plot=False):\n",
    "    height, width = images.shape[2] // patch_size, images.shape[3] // patch_size\n",
    "    images = images[:num_samples]\n",
    "    masks_enc = masks_enc[0][:num_samples]\n",
    "    masks_pred = torch.stack(masks_pred)\n",
    "    masks_pred = torch.unbind(masks_pred, dim=1)[:num_samples]\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=len(images), ncols=3, figsize=(10, len(images) * 4))\n",
    "    for i, (img, mask_indices, mask_indices_p) in enumerate(zip(images, masks_enc, masks_pred)):\n",
    "        if images.shape[1] > 3:\n",
    "            img = cutout_rgb(cutout=img.detach().cpu().numpy(), bands=bands, bands_rgb=['I', 'R', 'G'])\n",
    "            img = np.asarray(img, dtype=np.int32)\n",
    "            axs[i, 0].imshow(img)\n",
    "        else:\n",
    "            img = img.permute(1, 2, 0).detach().cpu().numpy()  # Change CxHxW to HxWxC for plotting\n",
    "            axs[i, 0].imshow(img)\n",
    "        \n",
    "        axs[i, 0].set_xlim(0, img.shape[0])\n",
    "        axs[i, 0].set_ylim(img.shape[1], 0)\n",
    "        axs[i, 0].set_title('Original Image')\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        if patch_grid:\n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 0].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 0].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "    \n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 1].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 1].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "\n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 2].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 2].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "        \n",
    "        full_mask = torch.zeros(height, width, dtype=torch.float32)\n",
    "        for idx in mask_indices:\n",
    "            row = idx // height\n",
    "            col = idx % width\n",
    "            full_mask[row, col] = 1\n",
    "        \n",
    "        part_masks_p = torch.zeros(mask_indices_p.shape[0], height, width, dtype=torch.float32)\n",
    "        for j, part_mask in enumerate(mask_indices_p):\n",
    "            for idx in part_mask:\n",
    "                row = idx // height\n",
    "                col = idx % width\n",
    "                part_masks_p[j, row, col] = 1\n",
    "        \n",
    "        # Upscale mask to match image resolution\n",
    "        full_mask = upscale_mask(full_mask, patch_size)\n",
    "\n",
    "        part_masks_p = torch.stack([upscale_mask(sub_mask, patch_size) for sub_mask in part_masks_p])\n",
    "        full_mask_p = torch.any(part_masks_p, dim=0)\n",
    "        \n",
    "        # Apply semi-transparent mask\n",
    "        masked_img = img.copy()\n",
    "        full_mask = full_mask.detach().cpu().numpy()\n",
    "        alpha = 0.6  # transparency level\n",
    "    \n",
    "        axs[i, 1].imshow(masked_img)\n",
    "        axs[i, 1].imshow(np.ma.masked_where(full_mask == 1, full_mask), cmap='cool', vmin=-1, alpha=alpha)\n",
    "        axs[i, 1].set_title('Final context mask')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        masked_img_p = img.copy()\n",
    "        full_mask_p = full_mask_p.detach().cpu().numpy()\n",
    "        \n",
    "        axs[i, 2].imshow(masked_img_p)\n",
    "        axs[i, 2].imshow(np.ma.masked_where(full_mask_p == 0, full_mask_p), cmap='cool', vmin=0, alpha=alpha, interpolation='none')\n",
    "        all_mask_boundaries = np.zeros(full_mask_p.shape, dtype='bool')\n",
    "        for k in range(part_masks_p.shape[0]):\n",
    "            mask_boundaries = find_boundaries(part_masks_p[k].detach().cpu().numpy(), mode='inner')\n",
    "            all_mask_boundaries |= mask_boundaries\n",
    "        axs[i, 2].imshow(np.ma.masked_where(all_mask_boundaries==0, all_mask_boundaries), cmap='cool', vmin=1, alpha=alpha, interpolation='none')\n",
    "        axs[i, 2].set_title('Target masks')\n",
    "        axs[i, 2].axis('off')\n",
    "    \n",
    "    if savename is not None:\n",
    "        plt.savefig(\n",
    "            savename,\n",
    "            bbox_inches='tight',\n",
    "            dpi=300,\n",
    "        )\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def cutout_rgb(cutout, bands, bands_rgb):\n",
    "    \"\"\"\n",
    "    Create an RGB image from the cutout data and save or plot it.\n",
    "\n",
    "    Args:\n",
    "        cutout (numpy.ndarray): array containing cutout data\n",
    "        bands (list): list of bands to use for the RGB image\n",
    "        in_dict (dict): band dictionary\n",
    "\n",
    "    Returns:\n",
    "        PIL image: image cutout\n",
    "    \"\"\"\n",
    "    band_idx = get_band_indices(bands, bands_rgb)[1]\n",
    "    cutout_rgb = cutout[band_idx]\n",
    "    # Currently assumes wrong band order [G, I, R, Y, Z]\n",
    "    cutout_red = cutout_rgb[1]  # R\n",
    "    cutout_green = cutout_rgb[2]  # G\n",
    "    cutout_blue = cutout_rgb[0]  # B\n",
    "\n",
    "    percentile = 99.9\n",
    "    saturation_percentile_threshold = 1000.0\n",
    "    high_saturation_threshold = 20000.0\n",
    "    interpolate_neg = False\n",
    "    min_size = 1000\n",
    "    percentile_red = np.nanpercentile(cutout_red, percentile)\n",
    "    percentile_green = np.nanpercentile(cutout_green, percentile)\n",
    "    percentile_blue = np.nanpercentile(cutout_blue, percentile)\n",
    "\n",
    "    #     print(f'{percentile} percentile r: {percentile_r}')\n",
    "    #     print(f'{percentile} percentile g: {percentile_g}')\n",
    "    #     print(f'{percentile} percentile i: {percentile_i}')\n",
    "\n",
    "    if np.any(\n",
    "        np.array([percentile_red, percentile_green, percentile_blue]) > saturation_percentile_threshold\n",
    "    ):\n",
    "        # If any band is highly saturated choose a lower percentile target to bring out more lsb features\n",
    "        if np.any(\n",
    "            np.array([percentile_red, percentile_green, percentile_blue]) > high_saturation_threshold\n",
    "        ):\n",
    "            percentile_target = 200.0\n",
    "        else:\n",
    "            percentile_target = 1000.0\n",
    "\n",
    "        # Find individual saturation percentiles for each band\n",
    "        percentiles = find_percentile_from_target(\n",
    "            [cutout_red, cutout_green, cutout_blue], percentile_target\n",
    "        )\n",
    "        cutout_red_desat, _ = desaturate(\n",
    "            cutout_red,\n",
    "            saturation_percentile=percentiles['R'],  # type: ignore\n",
    "            interpolate_neg=interpolate_neg,\n",
    "            min_size=min_size,\n",
    "        )\n",
    "        cutout_green_desat, _ = desaturate(\n",
    "            cutout_green,\n",
    "            saturation_percentile=percentiles['G'],  # type: ignore\n",
    "            interpolate_neg=interpolate_neg,\n",
    "            min_size=min_size,\n",
    "        )\n",
    "        cutout_blue_desat, _ = desaturate(\n",
    "            cutout_blue,\n",
    "            saturation_percentile=percentiles['B'],  # type: ignore\n",
    "            interpolate_neg=interpolate_neg,\n",
    "            min_size=min_size,\n",
    "        )\n",
    "\n",
    "        rgb = np.stack(\n",
    "            [cutout_red_desat, cutout_green_desat, cutout_blue_desat], axis=-1\n",
    "        )  # Stack data in [R, G, B] order\n",
    "    else:\n",
    "        rgb = np.stack([cutout_red, cutout_green, cutout_blue], axis=-1)\n",
    "\n",
    "    # Create RGB image\n",
    "    img_linear = rgb_image(\n",
    "        rgb,\n",
    "        scaling_type='linear',\n",
    "        stretch=0.9,\n",
    "        Q=5,\n",
    "        ceil_percentile=99.8,\n",
    "        dtype=np.uint8,\n",
    "        do_norm=True,\n",
    "        gamma=0.35,\n",
    "        scale_red=1.0,\n",
    "        scale_green=1.0,\n",
    "        scale_blue=1.0,\n",
    "    )\n",
    "\n",
    "    img_linear = Image.fromarray(img_linear)\n",
    "    img_linear = img_linear.transpose(Image.FLIP_TOP_BOTTOM)  # type: ignore\n",
    "\n",
    "    # obj_id = cutout['cfis_id'][obj_idx].decode('utf-8').replace(' ', '_')\n",
    "\n",
    "    # if save_rgb_cutout:\n",
    "    #     img_linear.save(os.path.join(save_dir, f'{obj_id}.png'))\n",
    "    # if plot_rgb_cutout:\n",
    "    #     plt.figure(figsize=(8, 8))\n",
    "    #     plt.imshow(img_linear)\n",
    "    #     plt.title(obj_id, fontsize=20)\n",
    "    #     plt.gca().axis('off')\n",
    "    #     plt.show()\n",
    "\n",
    "    return img_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86946d-7a15-42ce-b03b-afaf7651ede9",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337abc4-9487-40e4-a111-2b60dfa2999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(64, 64)\n",
    "patch_size=4\n",
    "enc_mask_scale=(0.85, 1.0)\n",
    "pred_mask_scale=(0.07, 0.1)\n",
    "aspect_ratio=(0.75, 1.5)\n",
    "nenc=1\n",
    "npred=6\n",
    "min_keep=15\n",
    "height, width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "alpha_grid = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3794ef-6b28-4f74-b907-a607d0a610f3",
   "metadata": {},
   "source": [
    "### Cutouts & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e51ab6-8317-4c2e-be44-5c06ca878b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/nick/astro/sky_embeddings/data'\n",
    "cutouts = read_h5(os.path.join(root,'HSC_dud_simple_classifier_data_GIRYZ7610_64.h5'))\n",
    "bands = ['G', 'I', 'R', 'Y', 'Z', 'NB0387', 'NB0816', 'NB0921', 'NB1010']\n",
    "# catalog = pd.read_csv(os.path.join(root,'(146, 324)_catalog.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddce3e-375b-4966-930a-70286e13c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutouts = read_h5(os.path.join(root,'146_324_224x224_ugriz.h5'))\n",
    "catalog = pd.read_csv(os.path.join(root,'(146, 324)_catalog.csv'))\n",
    "bands = ['U','G', 'R', 'I', 'Z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6013a-edd8-496f-a04c-23c9030e59ea",
   "metadata": {},
   "source": [
    "### Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce149-3fb5-4f9d-83b0-0986a7d050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = MaskCollator(\n",
    "input_size=input_size,\n",
    "patch_size=patch_size,\n",
    "pred_mask_scale=pred_mask_scale,\n",
    "enc_mask_scale=enc_mask_scale,\n",
    "aspect_ratio=aspect_ratio,\n",
    "nenc=nenc,\n",
    "npred=npred,\n",
    "allow_overlap=False,\n",
    "min_keep=min_keep)\n",
    " \n",
    "metadata = torch.from_numpy(np.column_stack((cutouts['ra'], cutouts['dec']))).to(torch.float32)\n",
    "\n",
    "dataset = RealTileDataset(cutouts=cutouts['cutouts'], metadata=metadata, crop_size=input_size)\n",
    "dataloader = torch.utils.data.DataLoader(  # type: ignore\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=mask_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dbcb3-a892-4bfa-a515-591b9d9bd9fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = []\n",
    "meta_list = []\n",
    "mask_enc_list = []\n",
    "mask_p_list = []\n",
    "\n",
    "for i, (images, meta, masks_enc, masks_p) in enumerate(dataloader):\n",
    "    image_list.append(images)\n",
    "    meta_list.append(meta)\n",
    "    mask_enc_list.append(masks_enc)\n",
    "    mask_p_list.append(masks_p)\n",
    "    print(f'shapes: images {images.shape}, metadata: {meta.shape}, mask_enc: {torch.stack(masks_enc).shape}, masks_pred: {torch.stack(masks_p).shape}')\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c75d-a1de-43a9-a358-212ff23737e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_in = load_images(batch_size=10, img_size=224)\n",
    "\n",
    "mask_collator = MaskCollator(\n",
    "input_size=input_size,\n",
    "patch_size=patch_size,\n",
    "pred_mask_scale=pred_mask_scale,\n",
    "enc_mask_scale=enc_mask_scale,\n",
    "aspect_ratio=aspect_ratio,\n",
    "nenc=nenc,\n",
    "npred=npred,\n",
    "allow_overlap=False,\n",
    "min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186995a6-ce0e-4505-99b7-9f335595fecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 8\n",
    "images, meta, masks_enc, masks_pred = image_list[idx], meta_list[idx], mask_enc_list[idx], mask_p_list[idx]\n",
    "# Just use one set of encoder masks for demonstration\n",
    "print(f'Shapes: images {images.shape}, masks_enc: {masks_enc[0].shape}, masks_pred {torch.stack(masks_pred).shape}')\n",
    "\n",
    "# images, meta, masks_enc, masks_p = mask_collator(images_in)\n",
    "# print(f'shapes: images: {images.shape}, meta: {meta.shape}, masks_enc: {masks_enc[0].shape}, masks_p: {torch.stack(masks_p).shape}')\n",
    "\n",
    "# stacked_masks_p = torch.stack(masks_pred)\n",
    "# reshaped_masks = torch.unbind(stacked_masks_p, dim=1)\n",
    "\n",
    "visualize_masks(images=images, masks_enc=masks_enc, masks_pred=masks_pred, patch_size=patch_size, bands=bands, alpha_grid=alpha_grid, num_samples=4, patch_grid=False, savename=None, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9dd9c-0e06-4281-b094-152c91a97636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
