{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ade69e3-5797-4e8a-926d-ea5df1fec01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from astropy.visualization import simple_norm\n",
    "from multiprocessing import Value\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from skimage.segmentation import find_boundaries\n",
    "import time\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.datastream_utils import band_dict_incl\n",
    "from utils.plotting import cutout_rgb, find_percentile_from_target\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "\n",
    "class MaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=(224, 224),\n",
    "        patch_size=16,\n",
    "        enc_mask_scale=(0.2, 0.8),\n",
    "        pred_mask_scale=(0.2, 0.8),\n",
    "        aspect_ratio=(0.3, 3.0),\n",
    "        nenc=1,\n",
    "        npred=2,\n",
    "        min_keep=4,\n",
    "        allow_overlap=False\n",
    "    ):\n",
    "        super(MaskCollator, self).__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  # minimum number of patches to keep\n",
    "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "        self.generator = torch.Generator()  # Initialize a generator\n",
    "        self.generator.manual_seed(int(time.time()))  # Seed the generator once\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        # -- Sample block scale\n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "        # -- Sample block aspect-ratio\n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        # -- Compute block height and width (given scale and aspect-ratio)\n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "        # --\n",
    "        # -- Loop to sample masks until we find a valid one\n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            # -- Sample block top-left corner\n",
    "            top = torch.randint(0, self.height - h + 1, (1,))\n",
    "            left = torch.randint(0, self.width - w + 1, (1,))\n",
    "            max_top = self.height - h\n",
    "            max_left = self.width - w\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            # -- Constrain mask to a set of acceptable regions\n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "            # -- If mask too small try again\n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    print(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        collated_batch = torch.utils.data.default_collate(batch)\n",
    "        \n",
    "        if len(collated_batch) == 2:\n",
    "            collated_images = collated_batch[0]\n",
    "            collated_metadata = collated_batch[1]\n",
    "        else:\n",
    "            collated_images = collated_batch\n",
    "            collated_metadata = torch.zeros(len(collated_batch))\n",
    "        B = len(batch)\n",
    "        \n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            \n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions = None\n",
    "            except Exception as e:\n",
    "                print(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_images, collated_metadata, collated_masks_enc, collated_masks_pred\n",
    "\n",
    "def load_images(batch_size=4):\n",
    "    # Transform to normalize the data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR10 images\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    # Fetch one batch of images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _ = next(dataiter)\n",
    "    return list(images)\n",
    "\n",
    "def upscale_mask(mask, patch_size):\n",
    "    upscaled_mask = mask.repeat_interleave(patch_size, dim=0)\n",
    "    upscaled_mask = upscaled_mask.repeat_interleave(patch_size, dim=1)\n",
    "    return upscaled_mask\n",
    "\n",
    "def visualize_masks(images, masks, masks_p, patch_size, alpha_grid, patch_grid=False):\n",
    "    fig, axs = plt.subplots(nrows=len(images), ncols=3, figsize=(10, len(images) * 4))\n",
    "    for i, (img, mask_indices, mask_indices_p) in enumerate(zip(images, masks, masks_p)):\n",
    "        if images.shape[1] == 5:\n",
    "            # only plot r-band\n",
    "            #img = img[2].detach().cpu().numpy()\n",
    "            #norm = simple_norm(img, 'sqrt', percent=98.)\n",
    "            #axs[i, 0].imshow(img, cmap='gray_r', norm=norm)\n",
    "\n",
    "            img = cutout_rgb(cutout=img.detach().cpu().numpy(), bands=['i', 'r', 'g'], in_dict=band_dict_incl)\n",
    "            img = np.asarray(img, dtype=np.int32)\n",
    "            axs[i, 0].imshow(img)\n",
    "        else:\n",
    "            img = img.permute(1, 2, 0).detach().cpu().numpy()  # Change CxHxW to HxWxC for plotting\n",
    "            axs[i, 0].imshow(img)\n",
    "        \n",
    "        axs[i, 0].set_xlim(0, img.shape[0])\n",
    "        axs[i, 0].set_ylim(img.shape[1], 0)\n",
    "        axs[i, 0].set_title('Original Image')\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        if patch_grid:\n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 0].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 0].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "    \n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 1].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 1].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "\n",
    "            for x in range(0, img.shape[0]+1, patch_size):\n",
    "                axs[i, 2].axvline(x, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "            for y in range(0, img.shape[1]+1, patch_size):\n",
    "                axs[i, 2].axhline(y, color='black', linestyle='--', lw=0.2, alpha=alpha_grid)  # Adjust color and line style if needed\n",
    "        \n",
    "        full_mask = torch.zeros(mask_collator.height, mask_collator.width, dtype=torch.float32)\n",
    "        for idx in mask_indices:\n",
    "            row = idx // mask_collator.width\n",
    "            col = idx % mask_collator.width\n",
    "            full_mask[row, col] = 1\n",
    "        \n",
    "        part_masks_p = torch.zeros(mask_indices_p.shape[0], mask_collator.height, mask_collator.width, dtype=torch.float32)\n",
    "        for j, part_mask in enumerate(mask_indices_p):\n",
    "            for idx in part_mask:\n",
    "                row = idx // mask_collator.width\n",
    "                col = idx % mask_collator.width\n",
    "                part_masks_p[j, row, col] = 1\n",
    "        \n",
    "        # Upscale mask to match image resolution\n",
    "        full_mask = upscale_mask(full_mask, mask_collator.patch_size)\n",
    "\n",
    "        part_masks_p = torch.stack([upscale_mask(sub_mask, mask_collator.patch_size) for sub_mask in part_masks_p])\n",
    "        full_mask_p = torch.any(part_masks_p, dim=0)\n",
    "        \n",
    "        # Apply semi-transparent mask\n",
    "        masked_img = img.copy()\n",
    "        full_mask = full_mask.detach().cpu().numpy()\n",
    "        alpha = 0.6  # transparency level\n",
    "        # color_mask = np.ones_like(img) * [1, 0, 0]  # red mask\n",
    "        # masked_img[full_mask == 0] = masked_img[full_mask == 0] * (1 - alpha) + color_mask[full_mask == 0] * alpha\n",
    "        if images.shape[1] == 5:\n",
    "            axs[i, 1].imshow(masked_img)\n",
    "        else:\n",
    "            axs[i, 1].imshow(masked_img)\n",
    "        axs[i, 1].imshow(np.ma.masked_where(full_mask == 1, full_mask), cmap='cool', vmin=-1, alpha=alpha)\n",
    "        axs[i, 1].set_title('Final context mask')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        masked_img_p = img.copy()\n",
    "        # masked_img_p[full_mask_p == 0] = masked_img_p[full_mask_p == 0] * (1 - alpha) + color_mask[full_mask_p == 0] * alpha\n",
    "        full_mask_p = full_mask_p.detach().cpu().numpy()\n",
    "        if images.shape[1] == 5:\n",
    "            axs[i, 2].imshow(masked_img_p)\n",
    "        else:\n",
    "            axs[i, 2].imshow(masked_img_p)\n",
    "        axs[i, 2].imshow(np.ma.masked_where(full_mask_p == 0, full_mask_p), cmap='cool', vmin=0, alpha=alpha, interpolation='none')\n",
    "        all_mask_boundaries = np.zeros(full_mask_p.shape, dtype='bool')\n",
    "        for k in range(part_masks_p.shape[0]):\n",
    "            mask_boundaries = find_boundaries(part_masks_p[k].detach().cpu().numpy(), mode='thin')\n",
    "            all_mask_boundaries |= mask_boundaries\n",
    "        axs[i, 2].imshow(np.ma.masked_where(all_mask_boundaries==0, all_mask_boundaries), cmap='cool', vmin=1, alpha=alpha, interpolation='none')\n",
    "        axs[i, 2].set_title('Target masks')\n",
    "        axs[i, 2].axis('off')\n",
    "    plt.show()\n",
    "    return img\n",
    "\n",
    "def read_h5(cutout_dir):\n",
    "    \"\"\"\n",
    "    Reads cutout data from HDF5 file\n",
    "    :param cutout_dir: cutout directory\n",
    "    :return: cutout data\n",
    "    \"\"\"\n",
    "    with h5py.File(cutout_dir, 'r') as f:\n",
    "        # Create empty dictionaries to store data for each group\n",
    "        cutout_data = {}\n",
    "\n",
    "        # Loop through datasets\n",
    "        for dataset_name in f:\n",
    "            data = np.array(f[dataset_name])\n",
    "            cutout_data[dataset_name] = data\n",
    "    return cutout_data\n",
    "\n",
    "class RealTileDataset(IterableDataset):\n",
    "    def __init__(self, cutouts, metadata):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with real data.\n",
    "\n",
    "        Args:\n",
    "        cutouts (Tensor): A tensor or list of image data.\n",
    "        metadata (Tensor): A tensor or list of corresponding metadata.\n",
    "        \"\"\"\n",
    "        catalog = tensor_compatible(metadata)\n",
    "        cutouts = torch.tensor(cutouts, dtype=torch.float32)\n",
    "        catalog = torch.tensor(catalog.values, dtype=torch.float32)\n",
    "        self.cutouts = cutouts\n",
    "        self.metadata = catalog\n",
    "\n",
    "    def __iter__(self):\n",
    "        for cutout, meta in zip(self.cutouts, self.metadata):\n",
    "            yield cutout, meta\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of items in the dataset\n",
    "        return len(self.cutouts)\n",
    "\n",
    "def convert_to_binary(input_string):\n",
    "    # Define the standard order of letters\n",
    "    standard_letters = 'ugriz'\n",
    "    # Initialize the result as an empty string\n",
    "    result = ''\n",
    "    # Iterate over each letter in the standard order\n",
    "    for letter in standard_letters:\n",
    "        # Append '2' if the letter is in the input string, '1' otherwise\n",
    "        if letter in input_string:\n",
    "            result += '2'\n",
    "        else:\n",
    "            result += '1'\n",
    "    # Convert the binary string to a decimal integer\n",
    "    return np.int32(result)\n",
    "\n",
    "\n",
    "def split_tile_nums(df):\n",
    "    # Split the tuple into two separate columns\n",
    "    if isinstance(df['tile'][0], str):\n",
    "        df['tile'] = df['tile'].apply(ast.literal_eval)\n",
    "    df['tile_num1'], df['tile_num2'] = zip(*df['tile'])\n",
    "    # Drop tile column\n",
    "    df.drop('tile', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tensor_compatible(df):\n",
    "    # Convert band info to integer number, 2 -> present, 1 -> absent\n",
    "    if isinstance(df['bands'][0], str):\n",
    "        df['bands'] = df['bands'].apply(convert_to_binary)\n",
    "    if 'tile' in df.columns:\n",
    "        # Split tile numbers up to two different columns and delete the tile column\n",
    "        df = split_tile_nums(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86946d-7a15-42ce-b03b-afaf7651ede9",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337abc4-9487-40e4-a111-2b60dfa2999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(224, 224)\n",
    "patch_size=8\n",
    "enc_mask_scale=(0.85, 1.0)\n",
    "pred_mask_scale=(0.02, 0.15)\n",
    "aspect_ratio=(0.5, 1.5)\n",
    "nenc=1\n",
    "npred=6\n",
    "min_keep=10\n",
    "height, width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "alpha_grid = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3794ef-6b28-4f74-b907-a607d0a610f3",
   "metadata": {},
   "source": [
    "### Cutouts & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e51ab6-8317-4c2e-be44-5c06ca878b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/nick/astro/sky_embeddings/data'\n",
    "cutouts = read_h5(os.path.join(root,'146_324_224x224_ugriz.h5'))\n",
    "catalog = pd.read_csv(os.path.join(root,'(146, 324)_catalog.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6013a-edd8-496f-a04c-23c9030e59ea",
   "metadata": {},
   "source": [
    "### Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce149-3fb5-4f9d-83b0-0986a7d050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = MaskCollator(\n",
    "input_size=input_size,\n",
    "patch_size=patch_size,\n",
    "pred_mask_scale=pred_mask_scale,\n",
    "enc_mask_scale=enc_mask_scale,\n",
    "aspect_ratio=aspect_ratio,\n",
    "nenc=nenc,\n",
    "npred=npred,\n",
    "allow_overlap=False,\n",
    "min_keep=min_keep)\n",
    "\n",
    "dataset = RealTileDataset(cutouts=cutouts['images'], metadata=catalog)\n",
    "dataloader = torch.utils.data.DataLoader(  # type: ignore\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=mask_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dbcb3-a892-4bfa-a515-591b9d9bd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "meta_list = []\n",
    "mask_enc_list = []\n",
    "mask_p_list = []\n",
    "\n",
    "for images, meta, masks_enc, masks_p in dataloader:\n",
    "    image_list.append(images)\n",
    "    meta_list.append(meta)\n",
    "    mask_enc_list.append(masks_enc)\n",
    "    mask_p_list.append(masks_p)\n",
    "    print(f'shapes: images {images.shape}, metadata: {meta.shape}, mask_enc: {torch.stack(masks_enc).shape}, masks_pred: {torch.stack(masks_p).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c75d-a1de-43a9-a358-212ff23737e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_in = load_images(batch_size=4)\n",
    "\n",
    "mask_collator = MaskCollator(\n",
    "input_size=input_size,\n",
    "patch_size=patch_size,\n",
    "pred_mask_scale=pred_mask_scale,\n",
    "enc_mask_scale=enc_mask_scale,\n",
    "aspect_ratio=aspect_ratio,\n",
    "nenc=nenc,\n",
    "npred=npred,\n",
    "allow_overlap=False,\n",
    "min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186995a6-ce0e-4505-99b7-9f335595fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images, meta, masks_enc, masks_p = mask_collator(images_in)\n",
    "#print(f'shapes: images: {images.shape}, meta: {meta.shape}, masks_enc: {masks_enc[0].shape}, masks_p: {torch.stack(masks_p).shape}')\n",
    "idx = 0\n",
    "images, meta, masks_enc, masks_p = image_list[idx], meta_list[idx], mask_enc_list[idx], mask_p_list[idx]\n",
    "stacked_masks_p = torch.stack(masks_p)\n",
    "reshaped_masks = torch.unbind(stacked_masks_p, dim=1)\n",
    "# Just use one set of encoder masks for demonstration\n",
    "img = visualize_masks(images, masks_enc[0], reshaped_masks, patch_size, alpha_grid, patch_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5984360-fe16-4dc3-adf4-bbfe98c507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "img_test = image_list[0][9].detach().cpu().numpy()\n",
    "band_idx = 4\n",
    "norm = simple_norm(img_test[band_idx], 'sqrt', percent=96.5)\n",
    "plt.imshow(img_test[band_idx], cmap='gray_r', norm=norm, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6ff63-ad9b-4100-ac11-bd67b3783bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[1][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911c692-52a8-4411-b1db-9d4e81383d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ac726-7335-44da-b4e9-e3d96e0b74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[1][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9dd9c-0e06-4281-b094-152c91a97636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
